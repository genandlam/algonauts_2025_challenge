{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3326e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "#import librosa\n",
    "import ast\n",
    "import string\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Button\n",
    "from IPython.display import Video, display, clear_output\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision.transforms import Compose, Lambda, CenterCrop\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale\n",
    "# Load the .mkv file\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e13aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on \"jupyter_notebook\" using \"cpu\" device!\n"
     ]
    }
   ],
   "source": [
    "root_data_dir = './data/algonauts_2025.competitors'\n",
    "# Select platform\n",
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook']\n",
    "initial_dir = os.getcwd() \n",
    "# Select device for computation\n",
    "device = 'cpu' # @param ['cpu', 'cuda']\n",
    "\n",
    "print(f'Running on \"{platform}\" using \"{device}\" device!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81446be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['ses-001_task-s01e02a', 'ses-001_task-s01e02b', 'ses-001_task-s01e03a', 'ses-001_task-s01e03b', 'ses-002_task-s01e04a', 'ses-002_task-s01e04b', 'ses-002_task-s01e05a', 'ses-002_task-s01e05b', 'ses-003_task-s01e01a', 'ses-003_task-s01e01b', 'ses-003_task-s01e06a', 'ses-003_task-s01e06b', 'ses-004_task-s01e07a', 'ses-004_task-s01e07b', 'ses-004_task-s01e08a', 'ses-004_task-s01e08b', 'ses-004_task-s01e09a', 'ses-004_task-s01e09b', 'ses-005_task-s01e10a', 'ses-005_task-s01e10b', 'ses-005_task-s01e11a', 'ses-005_task-s01e11b', 'ses-006_task-s01e12a', 'ses-006_task-s01e12b', 'ses-006_task-s01e13a', 'ses-006_task-s01e13b', 'ses-006_task-s01e14a', 'ses-006_task-s01e14b', 'ses-007_task-s01e15a', 'ses-007_task-s01e15b', 'ses-007_task-s01e16a', 'ses-007_task-s01e16b', 'ses-007_task-s01e17a', 'ses-007_task-s01e17b', 'ses-008_task-s01e18a', 'ses-008_task-s01e18b', 'ses-008_task-s01e19a', 'ses-008_task-s01e19b', 'ses-009_task-s01e20a', 'ses-009_task-s01e20b', 'ses-009_task-s01e21a', 'ses-009_task-s01e21b', 'ses-009_task-s01e22a', 'ses-009_task-s01e22b', 'ses-010_task-s01e23a', 'ses-010_task-s01e23b', 'ses-010_task-s01e24a', 'ses-010_task-s01e24b', 'ses-010_task-s02e01a', 'ses-010_task-s02e01b', 'ses-011_task-s02e02a', 'ses-011_task-s02e02b', 'ses-011_task-s02e03a', 'ses-011_task-s02e03b', 'ses-011_task-s02e04a', 'ses-011_task-s02e04b', 'ses-011_task-s02e05a', 'ses-011_task-s02e05b', 'ses-012_task-s02e06a', 'ses-012_task-s02e06b', 'ses-012_task-s02e07a', 'ses-012_task-s02e07b', 'ses-013_task-s02e08a', 'ses-013_task-s02e08b', 'ses-013_task-s02e09a', 'ses-013_task-s02e09b', 'ses-013_task-s02e10a', 'ses-013_task-s02e10b', 'ses-014_task-s02e11a', 'ses-014_task-s02e11b', 'ses-014_task-s02e12a', 'ses-014_task-s02e12b', 'ses-014_task-s02e13a', 'ses-014_task-s02e13b', 'ses-015_task-s02e14a', 'ses-015_task-s02e14b', 'ses-015_task-s02e15a', 'ses-015_task-s02e15b', 'ses-016_task-s02e16a', 'ses-016_task-s02e16b', 'ses-016_task-s02e17a', 'ses-016_task-s02e17b', 'ses-017_task-s02e18a', 'ses-017_task-s02e18b', 'ses-017_task-s02e19a', 'ses-017_task-s02e19b', 'ses-017_task-s02e20a', 'ses-017_task-s02e20b', 'ses-017_task-s02e21a', 'ses-017_task-s02e21b', 'ses-017_task-s02e22a', 'ses-017_task-s02e22b', 'ses-018_task-s02e23a', 'ses-018_task-s02e23b', 'ses-018_task-s02e24a', 'ses-018_task-s02e24b', 'ses-019_task-s03e01a', 'ses-019_task-s03e01b', 'ses-019_task-s03e02a', 'ses-019_task-s03e02b', 'ses-019_task-s03e03a', 'ses-019_task-s03e03b', 'ses-020_task-s03e04a', 'ses-020_task-s03e04b', 'ses-020_task-s03e05a', 'ses-020_task-s03e05b', 'ses-020_task-s03e06a', 'ses-020_task-s03e06b', 'ses-021_task-s03e07a', 'ses-021_task-s03e07b', 'ses-021_task-s03e08a', 'ses-021_task-s03e08b', 'ses-021_task-s03e09a', 'ses-021_task-s03e09b', 'ses-022_task-s03e10a', 'ses-022_task-s03e10b', 'ses-022_task-s03e11a', 'ses-022_task-s03e11b', 'ses-022_task-s03e12a', 'ses-022_task-s03e12b', 'ses-023_task-s03e13a', 'ses-023_task-s03e13b', 'ses-023_task-s03e14a', 'ses-023_task-s03e14b', 'ses-024_task-s03e15a', 'ses-024_task-s03e15b', 'ses-024_task-s03e16a', 'ses-024_task-s03e16b', 'ses-024_task-s03e17a', 'ses-024_task-s03e17b', 'ses-025_task-s03e18a', 'ses-025_task-s03e18b', 'ses-025_task-s03e19a', 'ses-025_task-s03e19b', 'ses-025_task-s03e20a', 'ses-025_task-s03e20b', 'ses-026_task-s03e21a', 'ses-026_task-s03e21b', 'ses-026_task-s03e22a', 'ses-026_task-s03e22b', 'ses-027_task-s03e23a', 'ses-027_task-s03e23b', 'ses-027_task-s03e24a', 'ses-027_task-s03e24b', 'ses-027_task-s03e25a', 'ses-027_task-s03e25b', 'ses-027_task-s04e01a', 'ses-027_task-s04e01b', 'ses-027_task-s04e02a', 'ses-027_task-s04e02b', 'ses-028_task-s04e03a', 'ses-028_task-s04e03b', 'ses-028_task-s04e04a', 'ses-028_task-s04e04b', 'ses-028_task-s04e05a', 'ses-028_task-s04e05b', 'ses-028_task-s04e06a', 'ses-028_task-s04e06b', 'ses-028_task-s04e07a', 'ses-028_task-s04e07b', 'ses-029_task-s04e08a', 'ses-029_task-s04e08b', 'ses-029_task-s04e09a', 'ses-029_task-s04e09b', 'ses-029_task-s04e10a', 'ses-029_task-s04e10b', 'ses-029_task-s04e11a', 'ses-029_task-s04e11b', 'ses-030_task-s04e12a', 'ses-030_task-s04e12b', 'ses-030_task-s04e13a', 'ses-030_task-s04e13b', 'ses-030_task-s04e14a', 'ses-030_task-s04e14b', 'ses-031_task-s04e15a', 'ses-031_task-s04e15b', 'ses-031_task-s04e16a', 'ses-031_task-s04e16b', 'ses-031_task-s04e17a', 'ses-031_task-s04e17b', 'ses-032_task-s04e18a', 'ses-032_task-s04e18b', 'ses-032_task-s04e19a', 'ses-032_task-s04e19b', 'ses-033_task-s04e20a', 'ses-033_task-s04e20b', 'ses-034_task-s04e21a', 'ses-034_task-s04e21b', 'ses-034_task-s04e22a', 'ses-034_task-s04e22b', 'ses-035_task-s04e23a', 'ses-035_task-s04e23b', 'ses-035_task-s04e23c', 'ses-035_task-s04e23d', 'ses-036_task-s05e01a', 'ses-036_task-s05e01b', 'ses-036_task-s05e02a', 'ses-036_task-s05e02b', 'ses-037_task-s05e03a', 'ses-037_task-s05e03b', 'ses-037_task-s05e04a', 'ses-037_task-s05e04b', 'ses-041_task-s05e05a', 'ses-041_task-s05e05b', 'ses-041_task-s05e06a', 'ses-041_task-s05e06b', 'ses-042_task-s05e07a', 'ses-042_task-s05e07b', 'ses-043_task-s05e08a', 'ses-043_task-s05e08b', 'ses-044_task-s05e09a', 'ses-044_task-s05e09b', 'ses-044_task-s05e10a', 'ses-044_task-s05e10b', 'ses-045_task-s05e11a', 'ses-045_task-s05e11b', 'ses-045_task-s05e12a', 'ses-045_task-s05e12b', 'ses-046_task-s05e13a', 'ses-046_task-s05e13b', 'ses-046_task-s05e14a', 'ses-046_task-s05e14b', 'ses-047_task-s05e15a', 'ses-047_task-s05e15b', 'ses-047_task-s05e16a', 'ses-047_task-s05e16b', 'ses-048_task-s05e17a', 'ses-048_task-s05e17b', 'ses-048_task-s05e18a', 'ses-048_task-s05e18b', 'ses-049_task-s05e19a', 'ses-049_task-s05e19b', 'ses-049_task-s05e20a', 'ses-049_task-s05e20b', 'ses-050_task-s05e21a', 'ses-050_task-s05e21b', 'ses-050_task-s05e22a', 'ses-050_task-s05e22b', 'ses-051_task-s05e23a', 'ses-051_task-s05e23b', 'ses-052_task-s05e23c', 'ses-052_task-s05e23d', 'ses-052_task-s06e01a', 'ses-052_task-s06e01b', 'ses-053_task-s06e02a', 'ses-053_task-s06e02b', 'ses-053_task-s06e03a', 'ses-053_task-s06e03b', 'ses-053_task-s06e04a', 'ses-054_task-s06e04b', 'ses-054_task-s06e05a', 'ses-054_task-s06e05b', 'ses-055_task-s06e06a', 'ses-055_task-s06e06b', 'ses-055_task-s06e07a', 'ses-055_task-s06e07b', 'ses-056_task-s06e08a', 'ses-056_task-s06e08b', 'ses-056_task-s06e09a', 'ses-056_task-s06e09b', 'ses-056_task-s06e10a', 'ses-057_task-s06e10b', 'ses-057_task-s06e11a', 'ses-057_task-s06e11b', 'ses-057_task-s06e12a', 'ses-057_task-s06e12b', 'ses-058_task-s06e13a', 'ses-058_task-s06e13b', 'ses-058_task-s06e14a', 'ses-058_task-s06e14b', 'ses-059_task-s06e15a', 'ses-060_task-s06e15b', 'ses-060_task-s06e15c', 'ses-061_task-s06e15d', 'ses-061_task-s06e17a', 'ses-061_task-s06e17b', 'ses-062_task-s06e18a', 'ses-062_task-s06e18b', 'ses-062_task-s06e19a', 'ses-062_task-s06e19b', 'ses-063_task-s06e20a', 'ses-063_task-s06e20b', 'ses-063_task-s06e21a', 'ses-064_task-s06e21b', 'ses-064_task-s06e22a', 'ses-064_task-s06e22b', 'ses-065_task-s06e23a', 'ses-065_task-s06e23b', 'ses-065_task-s06e24a', 'ses-066_task-s06e24b', 'ses-066_task-s06e24c', 'ses-066_task-s06e24d']\n",
      "Found:  ses-001_task-s01e02a\n"
     ]
    }
   ],
   "source": [
    "# Open the fMRI responses file, and extract the specific dataset \n",
    "fmri_file_path = root_data_dir + \"/fmri/sub-01/func/sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "\n",
    "# Open the H5 file in read mode\n",
    "with h5py.File(fmri_file_path, 'r') as file:\n",
    "    print(f\"Keys: {list(file.keys())}\")\n",
    "    a_group_key = list(file.keys())\n",
    "    for i in a_group_key:  \n",
    "        if \"s01e02a\" in i:\n",
    "            print(\"Found: \", i)\n",
    "            dataset_name = i\n",
    "            break\n",
    "    fmri_data = file[dataset_name][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c73b8",
   "metadata": {},
   "source": [
    "# audio features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043e172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[79333]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/py39/lib/python3.9/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x3184d43a8) and /opt/anaconda3/envs/py39/lib/libavdevice.59.7.100.dylib (0x3215d8778). One of the two will be used. Which one is undefined.\n",
      "objc[79333]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/py39/lib/python3.9/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x3184d43f8) and /opt/anaconda3/envs/py39/lib/libavdevice.59.7.100.dylib (0x3215d87c8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)\n",
    "\n",
    "def extract_audio_features(episode_path, tr,  device, save_dir_temp,\n",
    "    save_dir_features):\n",
    "    name_ep=episode_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    # Get the onset time of each movie chunk\n",
    "    clip = VideoFileClip(episode_path)\n",
    "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
    "    # Create the directory where the movie chunks are temporarily saved\n",
    "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    # Empty features list\n",
    "    audio_features = []\n",
    "    ### Loop over chunks ###\n",
    "    with tqdm(total=len(start_times), desc=\"Extracting audio features\") as pbar:\n",
    "        for start in start_times:\n",
    "\n",
    "            clip_chunk = clip.subclip(start, start+tr)\n",
    "            chunk_audio_path = os.path.join(temp_dir, 'audio_s01e01a.wav')\n",
    "            clip_chunk.audio.write_audiofile(chunk_audio_path, verbose=False,\n",
    "                            logger=None)\n",
    "            waveform, sample_rate = torchaudio.load(chunk_audio_path) \n",
    "            waveform = torch.mean(waveform, 0,True) # average the two channels of the wave files\n",
    "            waveform = waveform.to(device)\n",
    "            if sample_rate != bundle.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "            with torch.inference_mode():\n",
    "                features, _ = model.extract_features(waveform)\n",
    "                features = np.array(features, dtype='float32')\n",
    "                # keep last layer() of the model\n",
    "                features = features[11,:,:,:]\n",
    "                features = np.mean(features,1) # average the time frames \n",
    "                audio_features.append(features)\n",
    " \n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "    ### Convert the visual features to float32 ###\n",
    "    audio_features = np.array(audio_features, dtype='float32')\n",
    "    # Save the audio features\n",
    "    out_file_audio = os.path.join(\n",
    "        save_dir_features,name_ep+'_features_audio.h5')\n",
    "    print(f\"Saving audio features to {out_file_audio}\")\n",
    "    with h5py.File(out_file_audio, 'a' if Path(out_file_audio).exists() else 'w') as f:\n",
    "        group = f.create_group(name_ep)\n",
    "        group.create_dataset('audio', data=audio_features, dtype=np.float32)\n",
    "    print(f\"Audio features saved to {out_file_audio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f8c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_path(documents_path):\n",
    "    os.chdir(\"../algonauts_2025.competitors\")\n",
    "    paths=[glob.glob(os.path.join(documents_path, \"*.mkv\"))]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dff407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving audio features to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/audio_features/friends_features/\n",
      "Processing /Users/genevievelam/Documents/GitHub/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e09b.mkv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18d2a1dd8cf40e7abac895ea19a5ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting audio features:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s01e09b\n",
      "Saving audio features to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/audio_features/friends_features/s01e09b_features_audio.h5\n",
      "Audio features saved to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/audio_features/friends_features/s01e09b_features_audio.h5\n"
     ]
    }
   ],
   "source": [
    "# As an example, extract audio features using season 1, episode 1 of Friends\n",
    "\n",
    "show_path =[\"/stimuli/movies/friends/\",\"/stimuli/movies/movie10\"]\n",
    "documents_path = os.getcwd() +\"/stimuli/movies/friends/s1\"\n",
    "episode_paths = get_ep_path(documents_path)\n",
    "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
    "tr = 1.49\n",
    "# Saving directories\n",
    "save_dir_temp = initial_dir+\"/data/audio_features/\"\n",
    "save_dir_features = initial_dir+\"/data/audio_features/friends_features/\"\n",
    "print(f\"Saving audio features to {save_dir_features}\")\n",
    "\n",
    "# Execute audio feature extraction\n",
    "for i in episode_paths[0]:\n",
    "    extract_audio_features(i, tr, device,\n",
    "        save_dir_temp, save_dir_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_frames_transform():\n",
    "    \"\"\"Defines the preprocessing pipeline for the video frames. Note that this\n",
    "    transform is specific to the slow_r50 model.\"\"\"\n",
    "    transform = Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(8),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            Normalize([0.45, 0.45, 0.45], [0.225, 0.225, 0.225]),\n",
    "            ShortSideScale(size=256),\n",
    "            CenterCrop(256)\n",
    "        ]\n",
    "  )\n",
    "    return transform\n",
    "\n",
    "transform = define_frames_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9156c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/genevievelam/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "def get_vision_model(device):\n",
    "    \"\"\"\n",
    "    Load a pre-trained slow_r50 video model and set up the feature extractor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "        The device on which the model will run (i.e., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_extractor : torch.nn.Module\n",
    "        The feature extractor model.\n",
    "    model_layer : str\n",
    "        The layer from which visual features will be extracted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50',\n",
    "        pretrained=True)\n",
    "\n",
    "    # Select 'blocks.5.pool' as the feature extractor layer\n",
    "    model_layer = 'blocks.5.pool'\n",
    "    feature_extractor = create_feature_extractor(model,\n",
    "        return_nodes=[model_layer])\n",
    "    feature_extractor.to(device)\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    return feature_extractor, model_layer\n",
    "\n",
    "feature_extractor, model_layer = get_vision_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9296119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visual_features(episode_path, tr, feature_extractor, model_layer,\n",
    "    transform, device, save_dir_temp, save_dir_features):\n",
    "    \"\"\"\n",
    "    Extract visual features from a movie using a pre-trained video model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode_path : str\n",
    "        Path to the movie file for which the visual features are extracted.\n",
    "    tr : float\n",
    "        Duration of each chunk, in seconds (aligned with the fMRI repetition\n",
    "        time, or TR).\n",
    "    feature_extractor : torch.nn.Module\n",
    "        Pre-trained feature extractor model.\n",
    "    model_layer : str\n",
    "        The model layer from which the visual features are extracted.\n",
    "    transform : torchvision.transforms.Compose\n",
    "        Transformation pipeline for processing video frames.\n",
    "    device : torch.device\n",
    "        Device for computation ('cpu' or 'cuda').\n",
    "    save_dir_temp : str\n",
    "        Directory where the chunked movie clips are temporarily stored for\n",
    "        feature extraction.\n",
    "    save_dir_features : str\n",
    "        Directory where the extracted visual features are saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    visual_features : float\n",
    "        Array containing the extracted visual features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the onset time of each movie chunk\n",
    "    name_ep=episode_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    clip = VideoFileClip(episode_path)\n",
    "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
    "    # Create the directory where the movie chunks are temporarily saved\n",
    "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Empty features list\n",
    "    visual_features = []\n",
    "\n",
    "    # Loop over chunks\n",
    "    with tqdm(total=len(start_times), desc=\"Extracting visual features\") as pbar:\n",
    "        for start in start_times:\n",
    "\n",
    "            # Divide the movie in chunks of length TR, and save the resulting\n",
    "            # clips as '.mp4' files\n",
    "            clip_chunk = clip.subclip(start, start+tr)\n",
    "            chunk_path = os.path.join(temp_dir, 'visual_chunk.mp4')\n",
    "            clip_chunk.write_videofile(chunk_path, verbose=False, audio=False,\n",
    "                logger=None)\n",
    "            # Load the frames from the chunked movie clip\n",
    "            video_clip = VideoFileClip(chunk_path)\n",
    "            chunk_frames = [frame for frame in video_clip.iter_frames()]\n",
    "\n",
    "            # Format the frames to shape:\n",
    "            # (batch_size, channels, num_frames, height, width)\n",
    "            frames_array = np.transpose(np.array(chunk_frames), (3, 0, 1, 2))\n",
    "            # Convert the video frames to tensor\n",
    "            inputs = torch.from_numpy(frames_array).float()\n",
    "            # Preprocess the video frames\n",
    "            inputs = transform(inputs).unsqueeze(0).to(device)\n",
    "\n",
    "            # Extract the visual features\n",
    "            with torch.no_grad():\n",
    "                preds = feature_extractor(inputs)\n",
    "            visual_features.append(np.reshape(preds[model_layer].cpu().numpy(), -1))\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Convert the visual features to float32\n",
    "    visual_features = np.array(visual_features, dtype='float32')\n",
    "\n",
    "    # Save the visual features\n",
    "    out_file_visual = os.path.join(\n",
    "        save_dir_features, name_ep+'_features_visual.h5')\n",
    "    with h5py.File(out_file_visual, 'a' if Path(out_file_visual).exists() else 'w') as f:\n",
    "        group = f.create_group(name_ep)\n",
    "        group.create_dataset('visual', data=visual_features, dtype=np.float32)\n",
    "    print(f\"Visual features saved to {out_file_visual}\")\n",
    "\n",
    "    # Output\n",
    "    return visual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an exemple, extract visual features for season 1, episode 1 of Friends\n",
    "documents_path = os.getcwd() +\"/stimuli/movies/friends/s1\"\n",
    "episode_paths = get_ep_path(documents_path)\n",
    "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
    "tr = 1.49\n",
    "# Saving directories\n",
    "save_dir_temp = initial_dir+\"/data/visual_features/\"\n",
    "save_dir_features = initial_dir+\"/data/visual_features/friends_features/\"\n",
    "print(f\"Saving audio features to {save_dir_features}\")\n",
    "\n",
    "# Execute audio feature extraction\n",
    "for i in episode_paths[0][1:2]:\n",
    "    print(f\"Processing {i}\")\n",
    "    extract_visual_features(i, tr, feature_extractor,\n",
    "    model_layer, transform, device, save_dir_temp, save_dir_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
