{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3326e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "#import librosa\n",
    "import ast\n",
    "import string\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Button\n",
    "from IPython.display import Video, display, clear_output\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision.transforms import Compose, Lambda, CenterCrop\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale\n",
    "# Load the .mkv file\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e13aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on \"jupyter_notebook\" using \"cpu\" device!\n"
     ]
    }
   ],
   "source": [
    "root_data_dir = './data/algonauts_2025.competitors'\n",
    "# Select platform\n",
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook']\n",
    "initial_dir = os.getcwd() \n",
    "# Select device for computation\n",
    "device = 'cpu' # @param ['cpu', 'cuda']\n",
    "\n",
    "print(f'Running on \"{platform}\" using \"{device}\" device!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81446be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['ses-001_task-s01e02a', 'ses-001_task-s01e02b', 'ses-001_task-s01e03a', 'ses-001_task-s01e03b', 'ses-002_task-s01e04a', 'ses-002_task-s01e04b', 'ses-002_task-s01e05a', 'ses-002_task-s01e05b', 'ses-003_task-s01e01a', 'ses-003_task-s01e01b', 'ses-003_task-s01e06a', 'ses-003_task-s01e06b', 'ses-004_task-s01e07a', 'ses-004_task-s01e07b', 'ses-004_task-s01e08a', 'ses-004_task-s01e08b', 'ses-004_task-s01e09a', 'ses-004_task-s01e09b', 'ses-005_task-s01e10a', 'ses-005_task-s01e10b', 'ses-005_task-s01e11a', 'ses-005_task-s01e11b', 'ses-006_task-s01e12a', 'ses-006_task-s01e12b', 'ses-006_task-s01e13a', 'ses-006_task-s01e13b', 'ses-006_task-s01e14a', 'ses-006_task-s01e14b', 'ses-007_task-s01e15a', 'ses-007_task-s01e15b', 'ses-007_task-s01e16a', 'ses-007_task-s01e16b', 'ses-007_task-s01e17a', 'ses-007_task-s01e17b', 'ses-008_task-s01e18a', 'ses-008_task-s01e18b', 'ses-008_task-s01e19a', 'ses-008_task-s01e19b', 'ses-009_task-s01e20a', 'ses-009_task-s01e20b', 'ses-009_task-s01e21a', 'ses-009_task-s01e21b', 'ses-009_task-s01e22a', 'ses-009_task-s01e22b', 'ses-010_task-s01e23a', 'ses-010_task-s01e23b', 'ses-010_task-s01e24a', 'ses-010_task-s01e24b', 'ses-010_task-s02e01a', 'ses-010_task-s02e01b', 'ses-011_task-s02e02a', 'ses-011_task-s02e02b', 'ses-011_task-s02e03a', 'ses-011_task-s02e03b', 'ses-011_task-s02e04a', 'ses-011_task-s02e04b', 'ses-011_task-s02e05a', 'ses-011_task-s02e05b', 'ses-012_task-s02e06a', 'ses-012_task-s02e06b', 'ses-012_task-s02e07a', 'ses-012_task-s02e07b', 'ses-013_task-s02e08a', 'ses-013_task-s02e08b', 'ses-013_task-s02e09a', 'ses-013_task-s02e09b', 'ses-013_task-s02e10a', 'ses-013_task-s02e10b', 'ses-014_task-s02e11a', 'ses-014_task-s02e11b', 'ses-014_task-s02e12a', 'ses-014_task-s02e12b', 'ses-014_task-s02e13a', 'ses-014_task-s02e13b', 'ses-015_task-s02e14a', 'ses-015_task-s02e14b', 'ses-015_task-s02e15a', 'ses-015_task-s02e15b', 'ses-016_task-s02e16a', 'ses-016_task-s02e16b', 'ses-016_task-s02e17a', 'ses-016_task-s02e17b', 'ses-017_task-s02e18a', 'ses-017_task-s02e18b', 'ses-017_task-s02e19a', 'ses-017_task-s02e19b', 'ses-017_task-s02e20a', 'ses-017_task-s02e20b', 'ses-017_task-s02e21a', 'ses-017_task-s02e21b', 'ses-017_task-s02e22a', 'ses-017_task-s02e22b', 'ses-018_task-s02e23a', 'ses-018_task-s02e23b', 'ses-018_task-s02e24a', 'ses-018_task-s02e24b', 'ses-019_task-s03e01a', 'ses-019_task-s03e01b', 'ses-019_task-s03e02a', 'ses-019_task-s03e02b', 'ses-019_task-s03e03a', 'ses-019_task-s03e03b', 'ses-020_task-s03e04a', 'ses-020_task-s03e04b', 'ses-020_task-s03e05a', 'ses-020_task-s03e05b', 'ses-020_task-s03e06a', 'ses-020_task-s03e06b', 'ses-021_task-s03e07a', 'ses-021_task-s03e07b', 'ses-021_task-s03e08a', 'ses-021_task-s03e08b', 'ses-021_task-s03e09a', 'ses-021_task-s03e09b', 'ses-022_task-s03e10a', 'ses-022_task-s03e10b', 'ses-022_task-s03e11a', 'ses-022_task-s03e11b', 'ses-022_task-s03e12a', 'ses-022_task-s03e12b', 'ses-023_task-s03e13a', 'ses-023_task-s03e13b', 'ses-023_task-s03e14a', 'ses-023_task-s03e14b', 'ses-024_task-s03e15a', 'ses-024_task-s03e15b', 'ses-024_task-s03e16a', 'ses-024_task-s03e16b', 'ses-024_task-s03e17a', 'ses-024_task-s03e17b', 'ses-025_task-s03e18a', 'ses-025_task-s03e18b', 'ses-025_task-s03e19a', 'ses-025_task-s03e19b', 'ses-025_task-s03e20a', 'ses-025_task-s03e20b', 'ses-026_task-s03e21a', 'ses-026_task-s03e21b', 'ses-026_task-s03e22a', 'ses-026_task-s03e22b', 'ses-027_task-s03e23a', 'ses-027_task-s03e23b', 'ses-027_task-s03e24a', 'ses-027_task-s03e24b', 'ses-027_task-s03e25a', 'ses-027_task-s03e25b', 'ses-027_task-s04e01a', 'ses-027_task-s04e01b', 'ses-027_task-s04e02a', 'ses-027_task-s04e02b', 'ses-028_task-s04e03a', 'ses-028_task-s04e03b', 'ses-028_task-s04e04a', 'ses-028_task-s04e04b', 'ses-028_task-s04e05a', 'ses-028_task-s04e05b', 'ses-028_task-s04e06a', 'ses-028_task-s04e06b', 'ses-028_task-s04e07a', 'ses-028_task-s04e07b', 'ses-029_task-s04e08a', 'ses-029_task-s04e08b', 'ses-029_task-s04e09a', 'ses-029_task-s04e09b', 'ses-029_task-s04e10a', 'ses-029_task-s04e10b', 'ses-029_task-s04e11a', 'ses-029_task-s04e11b', 'ses-030_task-s04e12a', 'ses-030_task-s04e12b', 'ses-030_task-s04e13a', 'ses-030_task-s04e13b', 'ses-030_task-s04e14a', 'ses-030_task-s04e14b', 'ses-031_task-s04e15a', 'ses-031_task-s04e15b', 'ses-031_task-s04e16a', 'ses-031_task-s04e16b', 'ses-031_task-s04e17a', 'ses-031_task-s04e17b', 'ses-032_task-s04e18a', 'ses-032_task-s04e18b', 'ses-032_task-s04e19a', 'ses-032_task-s04e19b', 'ses-033_task-s04e20a', 'ses-033_task-s04e20b', 'ses-034_task-s04e21a', 'ses-034_task-s04e21b', 'ses-034_task-s04e22a', 'ses-034_task-s04e22b', 'ses-035_task-s04e23a', 'ses-035_task-s04e23b', 'ses-035_task-s04e23c', 'ses-035_task-s04e23d', 'ses-036_task-s05e01a', 'ses-036_task-s05e01b', 'ses-036_task-s05e02a', 'ses-036_task-s05e02b', 'ses-037_task-s05e03a', 'ses-037_task-s05e03b', 'ses-037_task-s05e04a', 'ses-037_task-s05e04b', 'ses-041_task-s05e05a', 'ses-041_task-s05e05b', 'ses-041_task-s05e06a', 'ses-041_task-s05e06b', 'ses-042_task-s05e07a', 'ses-042_task-s05e07b', 'ses-043_task-s05e08a', 'ses-043_task-s05e08b', 'ses-044_task-s05e09a', 'ses-044_task-s05e09b', 'ses-044_task-s05e10a', 'ses-044_task-s05e10b', 'ses-045_task-s05e11a', 'ses-045_task-s05e11b', 'ses-045_task-s05e12a', 'ses-045_task-s05e12b', 'ses-046_task-s05e13a', 'ses-046_task-s05e13b', 'ses-046_task-s05e14a', 'ses-046_task-s05e14b', 'ses-047_task-s05e15a', 'ses-047_task-s05e15b', 'ses-047_task-s05e16a', 'ses-047_task-s05e16b', 'ses-048_task-s05e17a', 'ses-048_task-s05e17b', 'ses-048_task-s05e18a', 'ses-048_task-s05e18b', 'ses-049_task-s05e19a', 'ses-049_task-s05e19b', 'ses-049_task-s05e20a', 'ses-049_task-s05e20b', 'ses-050_task-s05e21a', 'ses-050_task-s05e21b', 'ses-050_task-s05e22a', 'ses-050_task-s05e22b', 'ses-051_task-s05e23a', 'ses-051_task-s05e23b', 'ses-052_task-s05e23c', 'ses-052_task-s05e23d', 'ses-052_task-s06e01a', 'ses-052_task-s06e01b', 'ses-053_task-s06e02a', 'ses-053_task-s06e02b', 'ses-053_task-s06e03a', 'ses-053_task-s06e03b', 'ses-053_task-s06e04a', 'ses-054_task-s06e04b', 'ses-054_task-s06e05a', 'ses-054_task-s06e05b', 'ses-055_task-s06e06a', 'ses-055_task-s06e06b', 'ses-055_task-s06e07a', 'ses-055_task-s06e07b', 'ses-056_task-s06e08a', 'ses-056_task-s06e08b', 'ses-056_task-s06e09a', 'ses-056_task-s06e09b', 'ses-056_task-s06e10a', 'ses-057_task-s06e10b', 'ses-057_task-s06e11a', 'ses-057_task-s06e11b', 'ses-057_task-s06e12a', 'ses-057_task-s06e12b', 'ses-058_task-s06e13a', 'ses-058_task-s06e13b', 'ses-058_task-s06e14a', 'ses-058_task-s06e14b', 'ses-059_task-s06e15a', 'ses-060_task-s06e15b', 'ses-060_task-s06e15c', 'ses-061_task-s06e15d', 'ses-061_task-s06e17a', 'ses-061_task-s06e17b', 'ses-062_task-s06e18a', 'ses-062_task-s06e18b', 'ses-062_task-s06e19a', 'ses-062_task-s06e19b', 'ses-063_task-s06e20a', 'ses-063_task-s06e20b', 'ses-063_task-s06e21a', 'ses-064_task-s06e21b', 'ses-064_task-s06e22a', 'ses-064_task-s06e22b', 'ses-065_task-s06e23a', 'ses-065_task-s06e23b', 'ses-065_task-s06e24a', 'ses-066_task-s06e24b', 'ses-066_task-s06e24c', 'ses-066_task-s06e24d']\n",
      "Found:  ses-001_task-s01e02a\n"
     ]
    }
   ],
   "source": [
    "# Open the fMRI responses file, and extract the specific dataset \n",
    "fmri_file_path = root_data_dir + \"/fmri/sub-01/func/sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "\n",
    "# Open the H5 file in read mode\n",
    "with h5py.File(fmri_file_path, 'r') as file:\n",
    "    print(f\"Keys: {list(file.keys())}\")\n",
    "    a_group_key = list(file.keys())\n",
    "    for i in a_group_key:  \n",
    "        if \"s01e02a\" in i:\n",
    "            print(\"Found: \", i)\n",
    "            dataset_name = i\n",
    "            break\n",
    "    fmri_data = file[dataset_name][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c73b8",
   "metadata": {},
   "source": [
    "# audio features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043e172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[4022]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/py39/lib/python3.9/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x3163103a8) and /opt/anaconda3/envs/py39/lib/libavdevice.59.7.100.dylib (0x325ed8778). One of the two will be used. Which one is undefined.\n",
      "objc[4022]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/py39/lib/python3.9/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x3163103f8) and /opt/anaconda3/envs/py39/lib/libavdevice.59.7.100.dylib (0x325ed87c8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)\n",
    "\n",
    "def extract_audio_features(episode_path, tr,  device, save_dir_temp,\n",
    "    save_dir_features):\n",
    "    name_ep=episode_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    # Get the onset time of each movie chunk\n",
    "    clip = VideoFileClip(episode_path)\n",
    "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
    "    # Create the directory where the movie chunks are temporarily saved\n",
    "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    # Empty features list\n",
    "    audio_features = []\n",
    "    ### Loop over chunks ###\n",
    "    with tqdm(total=len(start_times), desc=\"Extracting audio features\") as pbar:\n",
    "        for start in start_times:\n",
    "\n",
    "            clip_chunk = clip.subclip(start, start+tr)\n",
    "            chunk_audio_path = os.path.join(temp_dir, 'audio_s01e01a.wav')\n",
    "            clip_chunk.audio.write_audiofile(chunk_audio_path, verbose=False,\n",
    "                            logger=None)\n",
    "            waveform, sample_rate = torchaudio.load(chunk_audio_path) \n",
    "            waveform = torch.mean(waveform, 0,True) # average the two channels of the wave files\n",
    "            waveform = waveform.to(device)\n",
    "            if sample_rate != bundle.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "            with torch.inference_mode():\n",
    "                features, _ = model.extract_features(waveform)\n",
    "                features = np.array(features, dtype='float32')\n",
    "                # keep last layer() of the model\n",
    "                features = features[11,:,:,:]\n",
    "                features = np.mean(features,1) # average the time frames \n",
    "                audio_features.append(features)\n",
    " \n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "    ### Convert the visual features to float32 ###\n",
    "    audio_features = np.array(audio_features, dtype='float32')\n",
    "    return audio_features\n",
    "    \"\"\"\n",
    "    # Save the audio features\n",
    "    out_file_audio = os.path.join(\n",
    "        save_dir_features,name_ep+'_features_audio.h5')\n",
    "    print(f\"Saving audio features to {out_file_audio}\")\n",
    "    with h5py.File(out_file_audio, 'a' if Path(out_file_audio).exists() else 'w') as f:\n",
    "        group = f.create_group(name_ep)\n",
    "        group.create_dataset('audio', data=audio_features, dtype=np.float32)\n",
    "    print(f\"Audio features saved to {out_file_audio}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f8c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_path(documents_path):\n",
    "    os.chdir(\"../algonauts_2025.competitors\")\n",
    "    paths=[glob.glob(os.path.join(documents_path, \"*.mkv\"))]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18dff407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving audio features to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/audio_features/friends_features/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4c84589e5f40788a9447d8fd63bfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting audio features:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Execute audio feature extraction\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m episode_paths[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mextract_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mextract_audio_features\u001b[0;34m(episode_path, tr, device, save_dir_temp, save_dir_features)\u001b[0m\n\u001b[1;32m     30\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(waveform, sample_rate, bundle\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 32\u001b[0m     features, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(features, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# keep last layer() of the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchaudio/models/wav2vec2/model.py:84\u001b[0m, in \u001b[0;36mWav2Vec2Model.extract_features\u001b[0;34m(self, waveforms, lengths, num_layers)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract feature vectors from raw waveforms\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mThis returns the list of outputs from the intermediate layers of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        It indicates the valid length in time axis of each feature Tensor.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m x, lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(waveforms, lengths)\n\u001b[0;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, lengths\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchaudio/models/wav2vec2/components.py:510\u001b[0m, in \u001b[0;36mEncoder.extract_features\u001b[0;34m(self, features, lengths, num_layers)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    505\u001b[0m     features: Tensor,\n\u001b[1;32m    506\u001b[0m     lengths: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    507\u001b[0m     num_layers: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tensor]:\n\u001b[1;32m    509\u001b[0m     x, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(features, lengths)\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intermediate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchaudio/models/wav2vec2/components.py:457\u001b[0m, in \u001b[0;36mTransformer.get_intermediate_outputs\u001b[0;34m(self, x, attention_mask, num_layers)\u001b[0m\n\u001b[1;32m    455\u001b[0m ret: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    456\u001b[0m position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    459\u001b[0m     x, position_bias \u001b[38;5;241m=\u001b[39m layer(x, attention_mask, position_bias\u001b[38;5;241m=\u001b[39mposition_bias)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchaudio/models/wav2vec2/components.py:422\u001b[0m, in \u001b[0;36mTransformer._preprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[0;32m--> 422\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_conv_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_first:\n\u001b[1;32m    425\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchaudio/models/wav2vec2/components.py:229\u001b[0m, in \u001b[0;36mConvolutionalPositionalEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    x (Tensor): shape ``[batch, frame, feature]``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    Tensor: The resulting feature. Shape ``[batch, frame, feature]``.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_remove \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_remove]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/utils/parametrize.py:407\u001b[0m, in \u001b[0;36m_inject_property.<locals>.get_parametrized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_cached_parametrization(parametrization)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# If caching is not active, this function just evaluates the parametrization\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparametrization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/utils/parametrize.py:303\u001b[0m, in \u001b[0;36mParametrizationList.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     originals \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mntensors))\n\u001b[0;32m--> 303\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moriginals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# It's not possible to call self[1:] here, so we have to be a bit more cryptic\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Also we want to skip all non-integer keys\u001b[39;00m\n\u001b[1;32m    306\u001b[0m curr_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/utils/parametrizations.py:325\u001b[0m, in \u001b[0;36m_WeightNorm.forward\u001b[0;34m(self, weight_g, weight_v)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight_g, weight_v):\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weight_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# As an example, extract audio features using season 1, episode 1 of Friends\n",
    "\n",
    "show_path =[\"/stimuli/movies/friends/\",\"/stimuli/movies/movie10\"]\n",
    "documents_path = os.getcwd() +\"/stimuli/movies/friends/s1\"\n",
    "episode_paths = get_ep_path(documents_path)\n",
    "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
    "tr = 1.49\n",
    "# Saving directories\n",
    "save_dir_temp = initial_dir+\"/data/audio_features/\"\n",
    "save_dir_features = initial_dir+\"/data/audio_features/friends_features/\"\n",
    "print(f\"Saving audio features to {save_dir_features}\")\n",
    "\n",
    "# Execute audio feature extraction\n",
    "for i in episode_paths[0][1:2]:\n",
    "    extract_audio_features(i, tr, device,\n",
    "        save_dir_temp, save_dir_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_frames_transform():\n",
    "    \"\"\"Defines the preprocessing pipeline for the video frames. Note that this\n",
    "    transform is specific to the slow_r50 model.\"\"\"\n",
    "    transform = Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(8),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            Normalize([0.45, 0.45, 0.45], [0.225, 0.225, 0.225]),\n",
    "            ShortSideScale(size=256),\n",
    "            CenterCrop(256)\n",
    "        ]\n",
    "  )\n",
    "    return transform\n",
    "\n",
    "transform = define_frames_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59d7a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['s01e05a']>\n",
      "<class 'h5py._hl.group.Group'>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "#Open the H5 file in read mode\n",
    "with h5py.File('/Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/audio_features/friends_features/s01e05a_features_audio.h5', 'r') as file:\n",
    "    print(\"Keys: %s\" % file.keys())\n",
    "    a_group_key = list(file.keys())[0]\n",
    "    \n",
    "     # get the object type for a_group_key: usually group or dataset\n",
    "    print(type(file[a_group_key])) \n",
    "\n",
    "    # If a_group_key is a group name, \n",
    "\n",
    "\n",
    "    # If a_group_key is a dataset name, \n",
    "    # this gets the dataset values and returns as a list\n",
    "    data = list(file[a_group_key])\n",
    "    # preferred methods to get dataset values:\n",
    "    ds_obj = file[a_group_key]      # returns as a h5py dataset object\n",
    "    #ds_arr = file[a_group_key][()]  # returns as a numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aded02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/genevievelam/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "def get_vision_model(device):\n",
    "    \"\"\"\n",
    "    Load a pre-trained slow_r50 video model and set up the feature extractor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "        The device on which the model will run (i.e., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_extractor : torch.nn.Module\n",
    "        The feature extractor model.\n",
    "    model_layer : str\n",
    "        The layer from which visual features will be extracted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50',\n",
    "        pretrained=True)\n",
    "\n",
    "    # Select 'blocks.5.pool' as the feature extractor layer\n",
    "    model_layer = 'blocks.5.pool'\n",
    "    feature_extractor = create_feature_extractor(model,\n",
    "        return_nodes=[model_layer])\n",
    "    feature_extractor.to(device)\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    return feature_extractor, model_layer\n",
    "\n",
    "feature_extractor, model_layer = get_vision_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9296119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visual_features(episode_path, tr, feature_extractor, model_layer,\n",
    "    transform, device, save_dir_temp, save_dir_features):\n",
    "    \"\"\"\n",
    "    Extract visual features from a movie using a pre-trained video model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode_path : str\n",
    "        Path to the movie file for which the visual features are extracted.\n",
    "    tr : float\n",
    "        Duration of each chunk, in seconds (aligned with the fMRI repetition\n",
    "        time, or TR).\n",
    "    feature_extractor : torch.nn.Module\n",
    "        Pre-trained feature extractor model.\n",
    "    model_layer : str\n",
    "        The model layer from which the visual features are extracted.\n",
    "    transform : torchvision.transforms.Compose\n",
    "        Transformation pipeline for processing video frames.\n",
    "    device : torch.device\n",
    "        Device for computation ('cpu' or 'cuda').\n",
    "    save_dir_temp : str\n",
    "        Directory where the chunked movie clips are temporarily stored for\n",
    "        feature extraction.\n",
    "    save_dir_features : str\n",
    "        Directory where the extracted visual features are saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    visual_features : float\n",
    "        Array containing the extracted visual features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the onset time of each movie chunk\n",
    "    name_ep=episode_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    clip = VideoFileClip(episode_path)\n",
    "    start_times = [x for x in np.arange(0, clip.duration, tr)][:-1]\n",
    "    # Create the directory where the movie chunks are temporarily saved\n",
    "    temp_dir = os.path.join(save_dir_temp, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Empty features list\n",
    "    visual_features = []\n",
    "\n",
    "    # Loop over chunks\n",
    "    with tqdm(total=len(start_times), desc=\"Extracting visual features\") as pbar:\n",
    "        for start in start_times:\n",
    "\n",
    "            # Divide the movie in chunks of length TR, and save the resulting\n",
    "            # clips as '.mp4' files\n",
    "            clip_chunk = clip.subclip(start, start+tr)\n",
    "            chunk_path = os.path.join(temp_dir, 'visual_chunk.mp4')\n",
    "            clip_chunk.write_videofile(chunk_path, verbose=False, audio=False,\n",
    "                logger=None)\n",
    "            # Load the frames from the chunked movie clip\n",
    "            video_clip = VideoFileClip(chunk_path)\n",
    "            chunk_frames = [frame for frame in video_clip.iter_frames()]\n",
    "\n",
    "            # Format the frames to shape:\n",
    "            # (batch_size, channels, num_frames, height, width)\n",
    "            frames_array = np.transpose(np.array(chunk_frames), (3, 0, 1, 2))\n",
    "            # Convert the video frames to tensor\n",
    "            inputs = torch.from_numpy(frames_array).float()\n",
    "            # Preprocess the video frames\n",
    "            inputs = transform(inputs).unsqueeze(0).to(device)\n",
    "\n",
    "            # Extract the visual features\n",
    "            with torch.no_grad():\n",
    "                preds = feature_extractor(inputs)\n",
    "            visual_features.append(np.reshape(preds[model_layer].cpu().numpy(), -1))\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Convert the visual features to float32\n",
    "    visual_features = np.array(visual_features, dtype='float32')\n",
    "\n",
    "    # Save the visual features\n",
    "    out_file_visual = os.path.join(\n",
    "        save_dir_features, name_ep+'_features_visual.h5')\n",
    "    with h5py.File(out_file_visual, 'a' if Path(out_file_visual).exists() else 'w') as f:\n",
    "        group = f.create_group(name_ep)\n",
    "        group.create_dataset('visual', data=visual_features, dtype=np.float32)\n",
    "    print(f\"Visual features saved to {out_file_visual}\")\n",
    "\n",
    "    # Output\n",
    "    return visual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving audio features to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/visual_features/friends_features/\n",
      "Processing /Users/genevievelam/Documents/GitHub/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e09b.mkv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224318f724d8431498ba6b301381e6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting visual features:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual features saved to /Users/genevievelam/Documents/GitHub/algonauts_2025_challenge/data/visual_features/friends_features/s01e09b_features_visual.h5\n"
     ]
    }
   ],
   "source": [
    "# As an exemple, extract visual features for season 1, episode 1 of Friends\n",
    "documents_path = os.getcwd() +\"/stimuli/movies/friends/s1\"\n",
    "episode_paths = get_ep_path(documents_path)\n",
    "# Duration of each movie chunk, aligned with the fMRI TR of 1.49 seconds\n",
    "tr = 1.49\n",
    "# Saving directories\n",
    "save_dir_temp = initial_dir+\"/data/visual_features/\"\n",
    "save_dir_features = initial_dir+\"/data/visual_features/friends_features/\"\n",
    "print(f\"Saving audio features to {save_dir_features}\")\n",
    "\n",
    "# Execute audio feature extraction\n",
    "for i in episode_paths[0]:\n",
    "    print(f\"Processing {i}\")\n",
    "    extract_visual_features(i, tr, feature_extractor,\n",
    "    model_layer, transform, device, save_dir_temp, save_dir_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f967594",
   "metadata": {},
   "source": [
    "# Aligning the visual and audio features with the fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_features_and_fmri_samples(features, fmri, excluded_samples_start,\n",
    "    excluded_samples_end, hrf_delay, stimulus_window, movies):\n",
    "    \"\"\"\n",
    "    Align the stimulus feature with the fMRI response samples for the selected\n",
    "    movies, later used to train and validate the encoding models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : dict\n",
    "        Dictionary containing the stimulus features.\n",
    "    fmri : dict\n",
    "        Dictionary containing the fMRI responses.\n",
    "    excluded_trs_start : int\n",
    "        Integer indicating the first N fMRI TRs that will be excluded and not\n",
    "        used for model training. The reason for excluding these TRs is that due\n",
    "        to the latency of the hemodynamic response the fMRI responses of first\n",
    "        few fMRI TRs do not yet contain stimulus-related information.\n",
    "    excluded_trs_end : int\n",
    "        Integer indicating the last N fMRI TRs that will be excluded and not\n",
    "        used for model training. The reason for excluding these TRs is that\n",
    "        stimulus feature samples (i.e., the stimulus chunks) can be shorter than\n",
    "        the fMRI samples (i.e., the fMRI TRs), since in some cases the fMRI run\n",
    "        ran longer than the actual movie. However, keep in mind that the fMRI\n",
    "        timeseries onset is ALWAYS SYNCHRONIZED with movie onset (i.e., the\n",
    "        first fMRI TR is always synchronized with the first stimulus chunk).\n",
    "    hrf_delay : int\n",
    "        fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
    "        that reflects changes in blood oxygenation levels in response to\n",
    "        activity in the brain. Blood flow increases to a given brain region in\n",
    "        response to its activity. This vascular response, which follows the\n",
    "        hemodynamic response function (HRF), takes time. Typically, the HRF\n",
    "        peaks around 5–6 seconds after a neural event: this delay reflects the\n",
    "        time needed for blood oxygenation changes to propagate and for the fMRI\n",
    "        signal to capture them. Therefore, this parameter introduces a delay\n",
    "        between stimulus chunks and fMRI samples for a better correspondence\n",
    "        between input stimuli and the brain response. For example, with a\n",
    "        hrf_delay of 3, if the stimulus chunk of interest is 17, the\n",
    "        corresponding fMRI sample will be 20.\n",
    "    stimulus_window : int\n",
    "        Integer indicating how many stimulus features' chunks are used to model\n",
    "        each fMRI TR, starting from the chunk corresponding to the TR of\n",
    "        interest, and going back in time. For example, with a stimulus_window of\n",
    "        5, if the fMRI TR of interest is 20, it will be modeled with stimulus\n",
    "        chunks [16, 17, 18, 19, 20]. Note that this only applies to visual and\n",
    "        audio features, since the language features were already extracted using\n",
    "        transcript words spanning several movie chunks (thus, each fMRI TR will\n",
    "        only be modeled using the corresponding language feature chunk). Also\n",
    "        note that a larger stimulus window will increase compute time, since it\n",
    "        increases the amount of stimulus features used to train and test the\n",
    "        fMRI encoding models.\n",
    "    movies: list\n",
    "        List of strings indicating the movies for which the fMRI responses and\n",
    "        stimulus features are aligned, out of the first six seasons of Friends\n",
    "        [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\",\n",
    "        \"friends-s05\", \"friends-s06\"], and the four movies from Movie10\n",
    "        [\"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned_features : float\n",
    "        Aligned stimulus features for the selected movies.\n",
    "    aligned_fmri : float\n",
    "        Aligned fMRI responses for the selected movies.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Empty data variables ###\n",
    "    aligned_features = []\n",
    "    aligned_fmri = np.empty((0,1000), dtype=np.float32)\n",
    "\n",
    "    ### Loop across movies ###\n",
    "    for movie in movies:\n",
    "\n",
    "        ### Get the IDs of all movies splits for the selected movie ###\n",
    "        if movie[:7] == 'friends':\n",
    "            id = movie[8:]\n",
    "        elif movie[:7] == 'movie10':\n",
    "            id = movie[8:]\n",
    "        movie_splits = [key for key in fmri if id in key[:len(id)]]\n",
    "\n",
    "        ### Loop over movie splits ###\n",
    "        for split in movie_splits:\n",
    "\n",
    "            ### Extract the fMRI ###\n",
    "            fmri_split = fmri[split]\n",
    "            # Exclude the first and last fMRI samples\n",
    "            fmri_split = fmri_split[excluded_samples_start:-excluded_samples_end]\n",
    "            aligned_fmri = np.append(aligned_fmri, fmri_split, 0)\n",
    "\n",
    "            ### Loop over fMRI samples ###\n",
    "            for s in range(len(fmri_split)):\n",
    "                # Empty variable containing the stimulus features of all\n",
    "                # modalities for each fMRI sample\n",
    "                f_all = np.empty(0)\n",
    "\n",
    "                ### Loop across modalities ###\n",
    "                for mod in features.keys():\n",
    "\n",
    "                    ### Visual and audio features ###\n",
    "                    # If visual or audio modality, model each fMRI sample using\n",
    "                    # the N stimulus feature samples up to the fMRI sample of\n",
    "                    # interest minus the hrf_delay (where N is defined by the\n",
    "                    # 'stimulus_window' variable)\n",
    "                    if mod == 'visual' or mod == 'audio':\n",
    "                        # In case there are not N stimulus feature samples up to\n",
    "                        # the fMRI sample of interest minus the hrf_delay (where\n",
    "                        # N is defined by the 'stimulus_window' variable), model\n",
    "                        # the fMRI sample using the first N stimulus feature\n",
    "                        # samples\n",
    "                        if s < (stimulus_window + hrf_delay):\n",
    "                            idx_start = excluded_samples_start\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        else:\n",
    "                            idx_start = s + excluded_samples_start - hrf_delay \\\n",
    "                                - stimulus_window + 1\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        # In case there are less visual/audio feature samples\n",
    "                        # than fMRI samples minus the hrf_delay, use the last N\n",
    "                        # visual/audio feature samples available (where N is\n",
    "                        # defined by the 'stimulus_window' variable)\n",
    "                        if idx_end > (len(features[mod][split])):\n",
    "                            idx_end = len(features[mod][split])\n",
    "                            idx_start = idx_end - stimulus_window\n",
    "                        f = features[mod][split][idx_start:idx_end]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                    ### Language features ###\n",
    "                    # Since language features already consist of embeddings\n",
    "                    # spanning several samples, only model each fMRI sample\n",
    "                    # using the corresponding stimulus feature sample minus the\n",
    "                    # hrf_delay\n",
    "                    elif mod == 'language':\n",
    "                        # In case there are no language features for the fMRI\n",
    "                        # sample of interest minus the hrf_delay, model the fMRI\n",
    "                        # sample using the first language feature sample\n",
    "                        if s < hrf_delay:\n",
    "                            idx = excluded_samples_start\n",
    "                        else:\n",
    "                            idx = s + excluded_samples_start - hrf_delay\n",
    "                        # In case there are fewer language feature samples than\n",
    "                        # fMRI samples minus the hrf_delay, use the last\n",
    "                        # language feature sample available\n",
    "                        if idx >= (len(features[mod][split]) - hrf_delay):\n",
    "                            f = features[mod][split][-1,:]\n",
    "                        else:\n",
    "                            f = features[mod][split][idx]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                 ### Append the stimulus features of all modalities for this sample ###\n",
    "                aligned_features.append(f_all)\n",
    "\n",
    "    ### Convert the aligned features to a numpy array ###\n",
    "    aligned_features = np.asarray(aligned_features, dtype=np.float32)\n",
    "\n",
    "    ### Output ###\n",
    "    return aligned_features, aligned_fmri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
