{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "#import librosa\n",
    "import ast\n",
    "import string\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Dropdown, Button\n",
    "from IPython.display import Video, display, clear_output\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision.transforms import Compose, Lambda, CenterCrop\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale\n",
    "# Load the .mkv file\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = './data/algonauts_2025.competitors'\n",
    "# Select platform\n",
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook']\n",
    "initial_dir = os.getcwd() \n",
    "# Select device for computation\n",
    "device = 'cpu' # @param ['cpu', 'cuda']\n",
    "\n",
    "print(f'Running on \"{platform}\" using \"{device}\" device!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,5]  #@param [\"1\", \"2\", \"3\", \"5\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "modality = \"all\"  #@param [\"visual\", \"audio\", \"language\", \"all\"]\n",
    "\n",
    "excluded_samples_start = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "\n",
    "excluded_samples_end = 1  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "\n",
    "hrf_delay = 3  #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "\n",
    "stimulus_window = 8  #@param {type:\"slider\", min:1, max:20, step:1}\n",
    "\n",
    "movies_train = [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\", \"friends-s05\", \"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"] # @param {allow-input: true}\n",
    "\n",
    "movies_val = [\"friends-s06\"] # @param {allow-input: true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stimulus_features(root_data_dir, modality, train_test):\n",
    "    \"\"\"\n",
    "    Load the stimulus features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_data_dir : str\n",
    "        Root data directory.\n",
    "    modality : str\n",
    "        Used feature modality.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        Dictionary containing the stimulus features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    ### Load the visual features ###\n",
    "    if modality == 'visual' or modality == 'all':\n",
    "        stimuli_dir = os.path.join(root_data_dir,'stimulus_features', 'pca',\n",
    "            'friends_movie10', 'visual', train_test)\n",
    "        features['visual'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
    "\n",
    "    ### Load the audio features ###\n",
    "    if modality == 'audio' or modality == 'all':\n",
    "        stimuli_dir = os.path.join(root_data_dir,'stimulus_features', 'pca',\n",
    "            'friends_movie10', 'audio', train_test)\n",
    "        features['audio'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
    "\n",
    "    ### Load the language features ###\n",
    "    if modality == 'language' or modality == 'all':\n",
    "        stimuli_dir = os.path.join(root_data_dir, 'stimulus_features', 'pca',\n",
    "            'friends_movie10', 'language', train_test)\n",
    "        features['language'] = np.load(stimuli_dir, allow_pickle=True).item()\n",
    "\n",
    "    ### Output ###\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stimulus features\n",
    "features = load_stimulus_features(initial_dir+\"/data/algonauts_2025.competitors\", modality,'features_train.npy')\n",
    "# Print all available movie splits for each stimulus modality\n",
    "\"\"\"\n",
    "for key_modality, value_modality in features.items():\n",
    "    print(f\"\\n{key_modality} features movie splits name and shape:\")\n",
    "    for key_movie, value_movie in value_modality.items():\n",
    "        print(key_movie + \" \" + str(value_movie.shape))     \n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb409c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmri(root_data_dir, subject):\n",
    "    \"\"\"\n",
    "    Load the fMRI responses for the selected subject.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_data_dir : str\n",
    "        Root data directory.\n",
    "    subject : int\n",
    "        Subject used to train and validate the encoding model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fmri : dict\n",
    "        Dictionary containing the  fMRI responses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fmri = {}\n",
    "\n",
    "    ### Load the fMRI responses for Friends ###\n",
    "    # Data directory\n",
    "    fmri_file = f'sub-0{subject}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5'\n",
    "                            #'_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir,#,'algonauts_2025.competitors',\n",
    "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
    "    # Load the the fMRI responses\n",
    "    fmri_friends = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_friends.items():\n",
    "        fmri[str(key[13:])] = val[:].astype(np.float32)\n",
    "    del fmri_friends\n",
    "\n",
    "    ### Load the fMRI responses for Movie10 ###\n",
    "    # Data directory\n",
    "    fmri_file = f'sub-0{subject}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir,#'algonauts_2025.competitors',\n",
    "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
    "    # Load the the fMRI responses\n",
    "    fmri_movie10 = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_movie10.items():\n",
    "        fmri[key[13:]] = val[:].astype(np.float32)\n",
    "    del fmri_movie10\n",
    "    # Average the fMRI responses across the two repeats for 'figures'\n",
    "    keys_all = fmri.keys()\n",
    "    figures_splits = 12\n",
    "    for s in range(figures_splits):\n",
    "        movie = 'figures' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "    # Average the fMRI responses across the two repeats for 'life'\n",
    "    keys_all = fmri.keys()\n",
    "    life_splits = 5\n",
    "    for s in range(life_splits):\n",
    "        movie = 'life' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "\n",
    "    ### Output ###\n",
    "    return fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf633f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fMRI responses\n",
    "fmri_subjects = {}\n",
    "for subject in subjects:\n",
    "    # Load the fMRI responses\n",
    "    fmri = load_fmri('/Users/genevievelam/Documents/GitHub/algonauts_2025.competitors/', subject)\n",
    "    fmri_subjects[subject] = fmri\n",
    "\"\"\"\n",
    "# Print all available movies\n",
    "print(f\"Subject {subject} fMRI movies splits name and shape:\")\n",
    "for key, value in fmri.items():\n",
    "    print(key + \" \" + str(value.shape))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51399be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_features_and_fmri_samples(features, fmri, excluded_samples_start,\n",
    "    excluded_samples_end, hrf_delay, stimulus_window, movies):\n",
    "    \"\"\"\n",
    "    Align the stimulus feature with the fMRI response samples for the selected\n",
    "    movies, later used to train and validate the encoding models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : dict\n",
    "        Dictionary containing the stimulus features.\n",
    "    fmri : dict\n",
    "        Dictionary containing the fMRI responses.\n",
    "    excluded_trs_start : int\n",
    "        Integer indicating the first N fMRI TRs that will be excluded and not\n",
    "        used for model training. The reason for excluding these TRs is that due\n",
    "        to the latency of the hemodynamic response the fMRI responses of first\n",
    "        few fMRI TRs do not yet contain stimulus-related information.\n",
    "    excluded_trs_end : int\n",
    "        Integer indicating the last N fMRI TRs that will be excluded and not\n",
    "        used for model training. The reason for excluding these TRs is that\n",
    "        stimulus feature samples (i.e., the stimulus chunks) can be shorter than\n",
    "        the fMRI samples (i.e., the fMRI TRs), since in some cases the fMRI run\n",
    "        ran longer than the actual movie. However, keep in mind that the fMRI\n",
    "        timeseries onset is ALWAYS SYNCHRONIZED with movie onset (i.e., the\n",
    "        first fMRI TR is always synchronized with the first stimulus chunk).\n",
    "    hrf_delay : int\n",
    "        fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
    "        that reflects changes in blood oxygenation levels in response to\n",
    "        activity in the brain. Blood flow increases to a given brain region in\n",
    "        response to its activity. This vascular response, which follows the\n",
    "        hemodynamic response function (HRF), takes time. Typically, the HRF\n",
    "        peaks around 5–6 seconds after a neural event: this delay reflects the\n",
    "        time needed for blood oxygenation changes to propagate and for the fMRI\n",
    "        signal to capture them. Therefore, this parameter introduces a delay\n",
    "        between stimulus chunks and fMRI samples for a better correspondence\n",
    "        between input stimuli and the brain response. For example, with a\n",
    "        hrf_delay of 3, if the stimulus chunk of interest is 17, the\n",
    "        corresponding fMRI sample will be 20.\n",
    "    stimulus_window : int\n",
    "        Integer indicating how many stimulus features' chunks are used to model\n",
    "        each fMRI TR, starting from the chunk corresponding to the TR of\n",
    "        interest, and going back in time. For example, with a stimulus_window of\n",
    "        5, if the fMRI TR of interest is 20, it will be modeled with stimulus\n",
    "        chunks [16, 17, 18, 19, 20]. Note that this only applies to visual and\n",
    "        audio features, since the language features were already extracted using\n",
    "        transcript words spanning several movie chunks (thus, each fMRI TR will\n",
    "        only be modeled using the corresponding language feature chunk). Also\n",
    "        note that a larger stimulus window will increase compute time, since it\n",
    "        increases the amount of stimulus features used to train and test the\n",
    "        fMRI encoding models.\n",
    "    movies: list\n",
    "        List of strings indicating the movies for which the fMRI responses and\n",
    "        stimulus features are aligned, out of the first six seasons of Friends\n",
    "        [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\",\n",
    "        \"friends-s05\", \"friends-s06\"], and the four movies from Movie10\n",
    "        [\"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned_features : float\n",
    "        Aligned stimulus features for the selected movies.\n",
    "    aligned_fmri : float\n",
    "        Aligned fMRI responses for the selected movies.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Empty data variables ###\n",
    "    aligned_features = []\n",
    "    aligned_fmri = np.empty((0,1000), dtype=np.float32)\n",
    "\n",
    "    ### Loop across movies ###\n",
    "    for movie in movies:\n",
    "\n",
    "        ### Get the IDs of all movies splits for the selected movie ###\n",
    "        if movie[:7] == 'friends':\n",
    "            id = movie[8:]\n",
    "        elif movie[:7] == 'movie10':\n",
    "            id = movie[8:]\n",
    "        movie_splits = [key for key in fmri if id in key[:len(id)]]\n",
    "\n",
    "        ### Loop over movie splits ###\n",
    "        for split in movie_splits:\n",
    "\n",
    "            ### Extract the fMRI ###\n",
    "            fmri_split = fmri[split]\n",
    "            # Exclude the first and last fMRI samples\n",
    "            fmri_split = fmri_split[excluded_samples_start:-excluded_samples_end]\n",
    "            aligned_fmri = np.append(aligned_fmri, fmri_split, 0)\n",
    "\n",
    "            ### Loop over fMRI samples ###\n",
    "            for s in range(len(fmri_split)):\n",
    "                # Empty variable containing the stimulus features of all\n",
    "                # modalities for each fMRI sample\n",
    "                f_all = np.empty(0)\n",
    "\n",
    "                ### Loop across modalities ###\n",
    "                for mod in features.keys():\n",
    "\n",
    "                    ### Visual and audio features ###\n",
    "                    # If visual or audio modality, model each fMRI sample using\n",
    "                    # the N stimulus feature samples up to the fMRI sample of\n",
    "                    # interest minus the hrf_delay (where N is defined by the\n",
    "                    # 'stimulus_window' variable)\n",
    "                    if mod == 'visual' or mod == 'audio':\n",
    "                        # In case there are not N stimulus feature samples up to\n",
    "                        # the fMRI sample of interest minus the hrf_delay (where\n",
    "                        # N is defined by the 'stimulus_window' variable), model\n",
    "                        # the fMRI sample using the first N stimulus feature\n",
    "                        # samples\n",
    "                        if s < (stimulus_window + hrf_delay):\n",
    "                            idx_start = excluded_samples_start\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        else:\n",
    "                            idx_start = s + excluded_samples_start - hrf_delay \\\n",
    "                                - stimulus_window + 1\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        # In case there are less visual/audio feature samples\n",
    "                        # than fMRI samples minus the hrf_delay, use the last N\n",
    "                        # visual/audio feature samples available (where N is\n",
    "                        # defined by the 'stimulus_window' variable)\n",
    "                        if idx_end > (len(features[mod][split])):\n",
    "                            idx_end = len(features[mod][split])\n",
    "                            idx_start = idx_end - stimulus_window\n",
    "                        f = features[mod][split][idx_start:idx_end]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                    ### Language features ###\n",
    "                    # Since language features already consist of embeddings\n",
    "                    # spanning several samples, only model each fMRI sample\n",
    "                    # using the corresponding stimulus feature sample minus the\n",
    "                    # hrf_delay\n",
    "                    elif mod == 'language':\n",
    "                        # In case there are no language features for the fMRI\n",
    "                        # sample of interest minus the hrf_delay, model the fMRI\n",
    "                        # sample using the first language feature sample\n",
    "                        if s < hrf_delay:\n",
    "                            idx = excluded_samples_start\n",
    "                        else:\n",
    "                            idx = s + excluded_samples_start - hrf_delay\n",
    "                        # In case there are fewer language feature samples than\n",
    "                        # fMRI samples minus the hrf_delay, use the last\n",
    "                        # language feature sample available\n",
    "                        if idx >= (len(features[mod][split]) - hrf_delay):\n",
    "                            f = features[mod][split][-1,:]\n",
    "                        else:\n",
    "                            f = features[mod][split][idx]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                 ### Append the stimulus features of all modalities for this sample ###\n",
    "                aligned_features.append(f_all)\n",
    "\n",
    "    ### Convert the aligned features to a numpy array ###\n",
    "    aligned_features = np.asarray(aligned_features, dtype=np.float32)\n",
    "\n",
    "    ### Output ###\n",
    "    return aligned_features, aligned_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in subjects:\n",
    "    features_train, fmri_train = align_features_and_fmri_samples(features, fmri_subjects[i],\n",
    "        excluded_samples_start, excluded_samples_end, hrf_delay,  stimulus_window,\n",
    "        movies_train)\n",
    "    if i == 1:\n",
    "        features_train_all=features_train\n",
    "        fmri_train_all=fmri_train\n",
    "    else:\n",
    "        features_train_all=np.concatenate((features_train_all, features_train), axis=0)\n",
    "        fmri_train_all=np.concatenate((fmri_train_all,fmri_train), dtype=np.float32)\n",
    "\n",
    "# Print the shape of the training fMRI responses and stimulus features: note\n",
    "# that the two have the same sample size!\n",
    "print(\"Training fMRI responses shape:\")\n",
    "print(fmri_train_all.shape)\n",
    "print('(Train samples × Parcels)')\n",
    "print(\"\\nTraining stimulus features shape:\")\n",
    "print(features_train_all.shape)\n",
    "print('(Train samples × Features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d95ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test3.npy', a)    # .npy extension is added if not given\n",
    "d = np.load('test3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def train_encoding(features_train, fmri_train):\n",
    "    \"\"\"\n",
    "    Train a linear-regression-based encoding model to predict fMRI responses\n",
    "    using movie features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_train : float\n",
    "        Stimulus features for the training movies.\n",
    "    fmri_train : float\n",
    "        fMRI responses for the training movies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : object\n",
    "        Trained regression model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Train the linear regression model ###\n",
    "    #model = LinearRegression().fit(features_train, fmri_train)\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    model = Ridge(random_state=16)\n",
    "\n",
    "    # fit the model with data\n",
    "    model.fit(features_train, fmri_train)\n",
    "    #from sklearn.ensemble import RandomForestRegressor\n",
    "    #model = RandomForestRegressor(random_state=0)\n",
    "    #model.fit(features_train, fmri_train)\n",
    "\n",
    "    ### Output ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05122053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the encoding model\n",
    "model = train_encoding(features_train_con, fmri_train_all_con)\n",
    "\n",
    "# Remove unused variables from memory\n",
    "#del features_train, fmri_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the stimulus features with the fMRI responses for the validation movies\n",
    "features_val_all = {}\n",
    "fmri_val_all = {}\n",
    "for i in [1]:\n",
    "    features_val, fmri_val = align_features_and_fmri_samples(features, fmri_subjects[1],\n",
    "        excluded_samples_start, excluded_samples_end, stimulus_window,\n",
    "        movies_val)\n",
    "    #features_val=delays(features_val)\n",
    "    features_val_all[i]=features_val\n",
    "    fmri_val_all[i]= fmri_val\n",
    "\n",
    "\n",
    "# Remove unused variables from memory\n",
    "#del features, fmri\n",
    "\n",
    "# Print the shape of the test fMRI responses and stimulus features: note\n",
    "# that the two have the same sample size!\n",
    "print(\"Validation fMRI responses shape:\", fmri_val_all[1].shape)\n",
    "print('(Validation samples × Parcels)')\n",
    "print(\"\\nValidation stimulus features shape:\", features_val_all[1].shape)\n",
    "print('(Validation samples × Features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022eb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the fMRI responses for the validation movies\n",
    "predictions = {}\n",
    "for i in [1]:\n",
    "    fmri_val_pred = model.predict(features_val_all[i])\n",
    "    predictions[i] = fmri_val_pred\n",
    "\n",
    "# Print the shape of the recorded and predicted test fMRI responses: note that\n",
    "# the two have the same shape!\n",
    "print(\"Validation fMRI responses shape:\", fmri_val_all[i].shape)\n",
    "print('(Validation samples × Parcels)')\n",
    "print(\"\\nValidation predicted fMRI responses shape:\", predictions[i].shape)\n",
    "print('(Validation samples × Parcels)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_encoding_accuracy(fmri_val, fmri_val_pred, subject, modality):\n",
    "    \"\"\"\n",
    "    Compare the  recorded (ground truth) and predicted fMRI responses, using a\n",
    "    Pearson's correlation. The comparison is perfomed independently for each\n",
    "    fMRI parcel. The correlation results are then plotted on a glass brain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fmri_val : float\n",
    "        fMRI responses for the validation movies.\n",
    "    fmri_val_pred : float\n",
    "        Predicted fMRI responses for the validation movies\n",
    "    subject : int\n",
    "        Subject number used to train and validate the encoding model.\n",
    "    modality : str\n",
    "        Feature modality used to train and validate the encoding model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Correlate recorded and predicted fMRI responses ###\n",
    "    encoding_accuracy = np.zeros((fmri_val.shape[1]), dtype=np.float32)\n",
    "    for p in range(len(encoding_accuracy)):\n",
    "        encoding_accuracy[p] = pearsonr(fmri_val[:, p],\n",
    "            fmri_val_pred[:, p])[0]\n",
    "    mean_encoding_accuracy = np.round(np.mean(encoding_accuracy), 3)\n",
    "\n",
    "    ### Map the prediction accuracy onto a 3D brain atlas for plotting ###\n",
    "    atlas_file = f'sub-0{subject}_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-dseg_parcellation.nii.gz'\n",
    "    atlas_path = os.path.join(initial_dir, 'data', 'algonauts_2025.competitors',\n",
    "        'fmri', f'sub-0{subject}', 'atlas', atlas_file)\n",
    "    atlas_masker = NiftiLabelsMasker(labels_img=atlas_path)\n",
    "    atlas_masker.fit()\n",
    "    encoding_accuracy_nii = atlas_masker.inverse_transform(encoding_accuracy)\n",
    "\n",
    "    ### Plot the encoding accuracy ###\n",
    "    title = f\"Encoding accuracy, sub-0{subject}, modality-{modality}, mean accuracy: \" + str(mean_encoding_accuracy)\n",
    "    display = plotting.plot_glass_brain(\n",
    "        encoding_accuracy_nii,\n",
    "        display_mode=\"lyrz\",\n",
    "        cmap='hot_r',\n",
    "        colorbar=True,\n",
    "        plot_abs=False,\n",
    "        symmetric_cbar=False,\n",
    "        title=title\n",
    "    )\n",
    "    colorbar = display._cbar\n",
    "    colorbar.set_label(\"Pearson's $r$\", rotation=90, labelpad=12, fontsize=12)\n",
    "    plotting.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_encoding_accuracy(fmri_val_all[1], predictions[1], 1, modality)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
